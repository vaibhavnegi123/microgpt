#include <cuda_runtime.h>

#include <algorithm>
#include <array>
#include <chrono>
#include <cmath>
#include <cctype>
#include <cstdlib>
#include <filesystem>
#include <fstream>
#include <iomanip>
#include <iostream>
#include <numeric>
#include <random>
#include <stdexcept>
#include <string>
#include <unordered_map>
#include <utility>
#include <vector>

/*
The most atomic way to train and inference a GPT on CUDA.
This file is the complete fused GPU algorithm.
Everything else is just efficiency.
*/

#define CUDA_CHECK(call)                                                                 \
    do {                                                                                 \
        cudaError_t err__ = (call);                                                      \
        if (err__ != cudaSuccess) {                                                      \
            throw std::runtime_error(std::string("CUDA error: ") +                      \
                                     cudaGetErrorString(err__) +                         \
                                     " at " + __FILE__ + ":" + std::to_string(__LINE__)); \
        }                                                                                \
    } while (0)

// Let there be fixed model geometry, matching microgpt.py exactly.
constexpr int kNEmbd = 16;           // embedding dimension
constexpr int kNHead = 4;            // number of attention heads
constexpr int kNLayer = 1;           // this fused kernel path currently supports one layer
constexpr int kBlockSize = 8;        // maximum sequence length
constexpr int kHeadDim = kNEmbd / kNHead;
constexpr int kFcDim = 4 * kNEmbd;   // hidden size of the MLP expansion
constexpr int kMaxVocab = 256;       // hard safety cap for static kernel buffers
constexpr int kMaxTokens = kBlockSize + 1; // [token_t, ..., token_{t+n}] for next-token targets

// Let there be training knobs, kept numerically aligned with the Python reference.
struct TrainConfig {
    int num_steps = 500;
    int val_every = 100;
    int val_docs = 20;
    int num_samples = 20;
    int top_k = 5;
    int seed = 42;
    float temperature = 0.6f;

    float learning_rate = 1e-2f;
    float beta1 = 0.9f;
    float beta2 = 0.95f;
    float eps_adam = 1e-8f;
    float weight_decay = 1e-4f;
    float max_grad_norm = 1.0f;
};

// Let there be dataset and tokenizer state.
struct DataBundle {
    std::vector<std::string> docs;
    std::vector<std::string> train_docs;
    std::vector<std::string> val_docs;
    std::unordered_map<char, int> stoi;
    std::vector<std::string> itos;
    int bos = 0;
};

// Let there be a tiny RAII wrapper over device memory.
template <typename T>
class DeviceBuffer {
public:
    DeviceBuffer() = default;
    explicit DeviceBuffer(size_t n) { resize(n); }

    ~DeviceBuffer() {
        if (ptr_ != nullptr) {
            cudaFree(ptr_);
            ptr_ = nullptr;
        }
    }

    DeviceBuffer(const DeviceBuffer&) = delete;
    DeviceBuffer& operator=(const DeviceBuffer&) = delete;

    DeviceBuffer(DeviceBuffer&& other) noexcept : ptr_(other.ptr_), size_(other.size_) {
        other.ptr_ = nullptr;
        other.size_ = 0;
    }

    DeviceBuffer& operator=(DeviceBuffer&& other) noexcept {
        if (this == &other) {
            return *this;
        }
        if (ptr_ != nullptr) {
            cudaFree(ptr_);
        }
        ptr_ = other.ptr_;
        size_ = other.size_;
        other.ptr_ = nullptr;
        other.size_ = 0;
        return *this;
    }

    void resize(size_t n) {
        if (n == size_) {
            return;
        }
        if (ptr_ != nullptr) {
            CUDA_CHECK(cudaFree(ptr_));
            ptr_ = nullptr;
        }
        size_ = n;
        if (size_ > 0) {
            CUDA_CHECK(cudaMalloc(&ptr_, sizeof(T) * size_));
        }
    }

    void upload(const std::vector<T>& host) {
        resize(host.size());
        if (size_ > 0) {
            CUDA_CHECK(cudaMemcpy(ptr_, host.data(), sizeof(T) * size_, cudaMemcpyHostToDevice));
        }
    }

    void upload_raw(const T* host, size_t n) {
        if (n > size_) {
            throw std::runtime_error("upload_raw exceeds device buffer size");
        }
        if (n > 0) {
            CUDA_CHECK(cudaMemcpy(ptr_, host, sizeof(T) * n, cudaMemcpyHostToDevice));
        }
    }

    void download(std::vector<T>& host) const {
        host.resize(size_);
        if (size_ > 0) {
            CUDA_CHECK(cudaMemcpy(host.data(), ptr_, sizeof(T) * size_, cudaMemcpyDeviceToHost));
        }
    }

    void download_raw(T* host, size_t n) const {
        if (n > size_) {
            throw std::runtime_error("download_raw exceeds device buffer size");
        }
        if (n > 0) {
            CUDA_CHECK(cudaMemcpy(host, ptr_, sizeof(T) * n, cudaMemcpyDeviceToHost));
        }
    }

    void zero() {
        if (size_ > 0) {
            CUDA_CHECK(cudaMemset(ptr_, 0, sizeof(T) * size_));
        }
    }

    T* data() { return ptr_; }
    const T* data() const { return ptr_; }
    size_t size() const { return size_; }

private:
    T* ptr_ = nullptr;
    size_t size_ = 0;
};

// Let there be a plain pointer view for passing model buffers into kernels.
struct ModelPtrs {
    int vocab_size = 0;

    float* wte = nullptr;
    float* wpe = nullptr;
    float* attn_wq = nullptr;
    float* attn_wk = nullptr;
    float* attn_wv = nullptr;
    float* attn_wo = nullptr;
    float* mlp_fc1 = nullptr;
    float* mlp_fc2 = nullptr;

    float* g_wte = nullptr;
    float* g_wpe = nullptr;
    float* g_attn_wq = nullptr;
    float* g_attn_wk = nullptr;
    float* g_attn_wv = nullptr;
    float* g_attn_wo = nullptr;
    float* g_mlp_fc1 = nullptr;
    float* g_mlp_fc2 = nullptr;

    float* m_wte = nullptr;
    float* m_wpe = nullptr;
    float* m_attn_wq = nullptr;
    float* m_attn_wk = nullptr;
    float* m_attn_wv = nullptr;
    float* m_attn_wo = nullptr;
    float* m_mlp_fc1 = nullptr;
    float* m_mlp_fc2 = nullptr;

    float* v_wte = nullptr;
    float* v_wpe = nullptr;
    float* v_attn_wq = nullptr;
    float* v_attn_wk = nullptr;
    float* v_attn_wv = nullptr;
    float* v_attn_wo = nullptr;
    float* v_mlp_fc1 = nullptr;
    float* v_mlp_fc2 = nullptr;
};

// Let there be parameters and optimizer state resident on the GPU.
struct DeviceModel {
    int vocab_size = 0;

    DeviceBuffer<float> wte;
    DeviceBuffer<float> wpe;
    DeviceBuffer<float> attn_wq;
    DeviceBuffer<float> attn_wk;
    DeviceBuffer<float> attn_wv;
    DeviceBuffer<float> attn_wo;
    DeviceBuffer<float> mlp_fc1;
    DeviceBuffer<float> mlp_fc2;

    DeviceBuffer<float> g_wte;
    DeviceBuffer<float> g_wpe;
    DeviceBuffer<float> g_attn_wq;
    DeviceBuffer<float> g_attn_wk;
    DeviceBuffer<float> g_attn_wv;
    DeviceBuffer<float> g_attn_wo;
    DeviceBuffer<float> g_mlp_fc1;
    DeviceBuffer<float> g_mlp_fc2;

    DeviceBuffer<float> m_wte;
    DeviceBuffer<float> m_wpe;
    DeviceBuffer<float> m_attn_wq;
    DeviceBuffer<float> m_attn_wk;
    DeviceBuffer<float> m_attn_wv;
    DeviceBuffer<float> m_attn_wo;
    DeviceBuffer<float> m_mlp_fc1;
    DeviceBuffer<float> m_mlp_fc2;

    DeviceBuffer<float> v_wte;
    DeviceBuffer<float> v_wpe;
    DeviceBuffer<float> v_attn_wq;
    DeviceBuffer<float> v_attn_wk;
    DeviceBuffer<float> v_attn_wv;
    DeviceBuffer<float> v_attn_wo;
    DeviceBuffer<float> v_mlp_fc1;
    DeviceBuffer<float> v_mlp_fc2;

    // Initialize weights with the same random scheme as microgpt.py.
    DeviceModel(int vocab, std::mt19937& rng)
        : vocab_size(vocab),
          wte(static_cast<size_t>(vocab) * kNEmbd),
          wpe(static_cast<size_t>(kBlockSize) * kNEmbd),
          attn_wq(static_cast<size_t>(kNEmbd) * kNEmbd),
          attn_wk(static_cast<size_t>(kNEmbd) * kNEmbd),
          attn_wv(static_cast<size_t>(kNEmbd) * kNEmbd),
          attn_wo(static_cast<size_t>(kNEmbd) * kNEmbd),
          mlp_fc1(static_cast<size_t>(kFcDim) * kNEmbd),
          mlp_fc2(static_cast<size_t>(kNEmbd) * kFcDim),
          g_wte(static_cast<size_t>(vocab) * kNEmbd),
          g_wpe(static_cast<size_t>(kBlockSize) * kNEmbd),
          g_attn_wq(static_cast<size_t>(kNEmbd) * kNEmbd),
          g_attn_wk(static_cast<size_t>(kNEmbd) * kNEmbd),
          g_attn_wv(static_cast<size_t>(kNEmbd) * kNEmbd),
          g_attn_wo(static_cast<size_t>(kNEmbd) * kNEmbd),
          g_mlp_fc1(static_cast<size_t>(kFcDim) * kNEmbd),
          g_mlp_fc2(static_cast<size_t>(kNEmbd) * kFcDim),
          m_wte(static_cast<size_t>(vocab) * kNEmbd),
          m_wpe(static_cast<size_t>(kBlockSize) * kNEmbd),
          m_attn_wq(static_cast<size_t>(kNEmbd) * kNEmbd),
          m_attn_wk(static_cast<size_t>(kNEmbd) * kNEmbd),
          m_attn_wv(static_cast<size_t>(kNEmbd) * kNEmbd),
          m_attn_wo(static_cast<size_t>(kNEmbd) * kNEmbd),
          m_mlp_fc1(static_cast<size_t>(kFcDim) * kNEmbd),
          m_mlp_fc2(static_cast<size_t>(kNEmbd) * kFcDim),
          v_wte(static_cast<size_t>(vocab) * kNEmbd),
          v_wpe(static_cast<size_t>(kBlockSize) * kNEmbd),
          v_attn_wq(static_cast<size_t>(kNEmbd) * kNEmbd),
          v_attn_wk(static_cast<size_t>(kNEmbd) * kNEmbd),
          v_attn_wv(static_cast<size_t>(kNEmbd) * kNEmbd),
          v_attn_wo(static_cast<size_t>(kNEmbd) * kNEmbd),
          v_mlp_fc1(static_cast<size_t>(kFcDim) * kNEmbd),
          v_mlp_fc2(static_cast<size_t>(kNEmbd) * kFcDim) {
        // Parameter init: Gaussian(0, 0.02), with selected projections zero-initialized.
        init_matrix(wte, 0.02f, rng);
        init_matrix(wpe, 0.02f, rng);
        init_matrix(attn_wq, 0.02f, rng);
        init_matrix(attn_wk, 0.02f, rng);
        init_matrix(attn_wv, 0.02f, rng);
        init_matrix(attn_wo, 0.0f, rng);
        init_matrix(mlp_fc1, 0.02f, rng);
        init_matrix(mlp_fc2, 0.0f, rng);

        g_wte.zero();
        g_wpe.zero();
        g_attn_wq.zero();
        g_attn_wk.zero();
        g_attn_wv.zero();
        g_attn_wo.zero();
        g_mlp_fc1.zero();
        g_mlp_fc2.zero();

        m_wte.zero();
        m_wpe.zero();
        m_attn_wq.zero();
        m_attn_wk.zero();
        m_attn_wv.zero();
        m_attn_wo.zero();
        m_mlp_fc1.zero();
        m_mlp_fc2.zero();

        v_wte.zero();
        v_wpe.zero();
        v_attn_wq.zero();
        v_attn_wk.zero();
        v_attn_wv.zero();
        v_attn_wo.zero();
        v_mlp_fc1.zero();
        v_mlp_fc2.zero();
    }

    // Count all trainable scalars, for observability parity with microgpt.py.
    size_t num_params() const {
        return wte.size() + wpe.size() + attn_wq.size() + attn_wk.size() + attn_wv.size() +
               attn_wo.size() + mlp_fc1.size() + mlp_fc2.size();
    }

    ModelPtrs ptrs() {
        ModelPtrs p;
        p.vocab_size = vocab_size;

        p.wte = wte.data();
        p.wpe = wpe.data();
        p.attn_wq = attn_wq.data();
        p.attn_wk = attn_wk.data();
        p.attn_wv = attn_wv.data();
        p.attn_wo = attn_wo.data();
        p.mlp_fc1 = mlp_fc1.data();
        p.mlp_fc2 = mlp_fc2.data();

        p.g_wte = g_wte.data();
        p.g_wpe = g_wpe.data();
        p.g_attn_wq = g_attn_wq.data();
        p.g_attn_wk = g_attn_wk.data();
        p.g_attn_wv = g_attn_wv.data();
        p.g_attn_wo = g_attn_wo.data();
        p.g_mlp_fc1 = g_mlp_fc1.data();
        p.g_mlp_fc2 = g_mlp_fc2.data();

        p.m_wte = m_wte.data();
        p.m_wpe = m_wpe.data();
        p.m_attn_wq = m_attn_wq.data();
        p.m_attn_wk = m_attn_wk.data();
        p.m_attn_wv = m_attn_wv.data();
        p.m_attn_wo = m_attn_wo.data();
        p.m_mlp_fc1 = m_mlp_fc1.data();
        p.m_mlp_fc2 = m_mlp_fc2.data();

        p.v_wte = v_wte.data();
        p.v_wpe = v_wpe.data();
        p.v_attn_wq = v_attn_wq.data();
        p.v_attn_wk = v_attn_wk.data();
        p.v_attn_wv = v_attn_wv.data();
        p.v_attn_wo = v_attn_wo.data();
        p.v_mlp_fc1 = v_mlp_fc1.data();
        p.v_mlp_fc2 = v_mlp_fc2.data();
        return p;
    }

private:
    // Host-side random init, uploaded once; parameters stay device-resident afterward.
    static void init_matrix(DeviceBuffer<float>& dst, float stddev, std::mt19937& rng) {
        std::vector<float> host(dst.size(), 0.0f);
        if (stddev != 0.0f) {
            std::normal_distribution<float> dist(0.0f, stddev);
            for (float& x : host) {
                x = dist(rng);
            }
        }
        dst.upload(host);
    }
};

// Trim a line from the input corpus.
static std::string trim_copy(const std::string& s) {
    size_t start = 0;
    while (start < s.size() && std::isspace(static_cast<unsigned char>(s[start]))) {
        ++start;
    }
    size_t end = s.size();
    while (end > start && std::isspace(static_cast<unsigned char>(s[end - 1]))) {
        --end;
    }
    return s.substr(start, end - start);
}

// Let there be an input dataset file, matching Python behavior on first run.
static void ensure_input_file() {
    namespace fs = std::filesystem;
    if (fs::exists("input.txt")) {
        return;
    }
#ifdef _WIN32
    const char* cmd =
        "powershell -NoProfile -Command \""
        "$u='https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt';"
        "Invoke-WebRequest -Uri $u -OutFile 'input.txt' -UseBasicParsing\"";
    int rc = std::system(cmd);
    if (rc != 0 || !fs::exists("input.txt")) {
        throw std::runtime_error("failed to download input.txt");
    }
#else
    throw std::runtime_error("input.txt missing and auto-download only implemented for Windows");
#endif
}

// Load docs, split train/val, and build a character tokenizer with <BOS>.
static DataBundle load_data(std::mt19937& rng) {
    ensure_input_file();

    std::ifstream f("input.txt");
    if (!f) {
        throw std::runtime_error("failed to open input.txt");
    }

    DataBundle data;
    std::string line;
    while (std::getline(f, line)) {
        std::string s = trim_copy(line);
        if (!s.empty()) {
            data.docs.push_back(s);
        }
    }
    if (data.docs.empty()) {
        throw std::runtime_error("input.txt is empty");
    }

    std::shuffle(data.docs.begin(), data.docs.end(), rng);
    size_t split = static_cast<size_t>(0.9 * static_cast<double>(data.docs.size()));
    if (split == 0 && data.docs.size() > 1) {
        split = 1;
    }
    if (split >= data.docs.size() && data.docs.size() > 1) {
        split = data.docs.size() - 1;
    }
    data.train_docs.assign(data.docs.begin(), data.docs.begin() + static_cast<std::ptrdiff_t>(split));
    data.val_docs.assign(data.docs.begin() + static_cast<std::ptrdiff_t>(split), data.docs.end());

    std::array<bool, 256> seen{};
    for (const std::string& doc : data.docs) {
        for (unsigned char c : doc) {
            seen[c] = true;
        }
    }

    std::vector<char> chars;
    for (int i = 0; i < 256; ++i) {
        if (seen[static_cast<size_t>(i)]) {
            chars.push_back(static_cast<char>(i));
        }
    }
    std::sort(chars.begin(), chars.end());

    data.itos.clear();
    data.itos.push_back("<BOS>");
    data.bos = 0;
    for (char c : chars) {
        data.stoi[c] = static_cast<int>(data.itos.size());
        data.itos.push_back(std::string(1, c));
    }
    return data;
}

// Encode one document as [BOS] + chars + [BOS], exactly like microgpt.py.
static std::vector<int> encode_doc(
    const std::string& doc,
    const std::unordered_map<char, int>& stoi,
    int bos_token) {
    std::vector<int> tokens;
    tokens.reserve(doc.size() + 2);
    tokens.push_back(bos_token);
    for (char c : doc) {
        auto it = stoi.find(c);
        if (it == stoi.end()) {
            throw std::runtime_error("document contains out-of-vocab character");
        }
        tokens.push_back(it->second);
    }
    tokens.push_back(bos_token);
    return tokens;
}

// Top-k sampling with temperature, used at inference time.
static int sample_top_k(
    const std::vector<float>& logits,
    int top_k,
    float temperature,
    std::mt19937& rng) {
    int vocab = static_cast<int>(logits.size());
    int k = std::max(1, std::min(top_k, vocab));
    std::vector<int> ids(static_cast<size_t>(vocab));
    std::iota(ids.begin(), ids.end(), 0);
    std::partial_sort(
        ids.begin(),
        ids.begin() + k,
        ids.end(),
        [&](int a, int b) { return logits[static_cast<size_t>(a)] > logits[static_cast<size_t>(b)]; });

    std::vector<float> scaled(static_cast<size_t>(k), 0.0f);
    for (int i = 0; i < k; ++i) {
        scaled[static_cast<size_t>(i)] = logits[static_cast<size_t>(ids[static_cast<size_t>(i)])] / temperature;
    }
    float mx = *std::max_element(scaled.begin(), scaled.end());
    std::vector<float> probs(static_cast<size_t>(k), 0.0f);
    for (int i = 0; i < k; ++i) {
        probs[static_cast<size_t>(i)] = std::exp(scaled[static_cast<size_t>(i)] - mx);
    }
    std::discrete_distribution<int> dist(probs.begin(), probs.end());
    return ids[static_cast<size_t>(dist(rng))];
}

// Parse optional CLI overrides for fast experiments.
static TrainConfig parse_args(int argc, char** argv) {
    TrainConfig cfg;
    for (int i = 1; i < argc; ++i) {
        std::string arg = argv[i];
        auto read_int = [&](const char* name) -> int {
            if (i + 1 >= argc) {
                throw std::runtime_error(std::string("missing value for ") + name);
            }
            return std::stoi(argv[++i]);
        };
        auto read_float = [&](const char* name) -> float {
            if (i + 1 >= argc) {
                throw std::runtime_error(std::string("missing value for ") + name);
            }
            return std::stof(argv[++i]);
        };

        if (arg == "--steps") {
            cfg.num_steps = read_int("--steps");
        } else if (arg == "--val-every") {
            cfg.val_every = read_int("--val-every");
        } else if (arg == "--val-docs") {
            cfg.val_docs = read_int("--val-docs");
        } else if (arg == "--samples") {
            cfg.num_samples = read_int("--samples");
        } else if (arg == "--top-k") {
            cfg.top_k = read_int("--top-k");
        } else if (arg == "--temperature") {
            cfg.temperature = read_float("--temperature");
        } else if (arg == "--seed") {
            cfg.seed = read_int("--seed");
        } else if (arg == "--help") {
            std::cout
                << "Usage: microgpt_cuda [options]\n"
                << "  --steps <int>         training steps (default 500)\n"
                << "  --val-every <int>     validation interval (default 100)\n"
                << "  --val-docs <int>      max validation docs per eval (default 20)\n"
                << "  --samples <int>       number of generated samples (default 20)\n"
                << "  --top-k <int>         top-k sampling (default 5)\n"
                << "  --temperature <float> sampling temperature (default 0.6)\n"
                << "  --seed <int>          RNG seed (default 42)\n";
            std::exit(0);
        } else {
            throw std::runtime_error("unknown argument: " + arg);
        }
    }
    return cfg;
}

// Below live the scalar CUDA building blocks used by fused kernels.
__device__ inline void d_vec_copy(const float* src, float* dst, int n) {
    for (int i = 0; i < n; ++i) {
        dst[i] = src[i];
    }
}

__device__ inline void d_vec_add_inplace(float* dst, const float* src, int n) {
    for (int i = 0; i < n; ++i) {
        dst[i] += src[i];
    }
}

// y = W * x
__device__ inline void d_matvec(const float* w, const float* x, float* y, int out, int in) {
    for (int row = 0; row < out; ++row) {
        float acc = 0.0f;
        int base = row * in;
        for (int col = 0; col < in; ++col) {
            acc += w[base + col] * x[col];
        }
        y[row] = acc;
    }
}

// dx = W^T * dy
__device__ inline void d_matvec_t(const float* w, const float* dy, float* dx, int out, int in) {
    for (int col = 0; col < in; ++col) {
        float acc = 0.0f;
        for (int row = 0; row < out; ++row) {
            acc += w[row * in + col] * dy[row];
        }
        dx[col] = acc;
    }
}

// dW += dy outer x
__device__ inline void d_outer_add(float* dw, const float* dy, const float* x, int out, int in) {
    for (int row = 0; row < out; ++row) {
        int base = row * in;
        for (int col = 0; col < in; ++col) {
            dw[base + col] += dy[row] * x[col];
        }
    }
}

// Fused linear backward: accumulate dW and produce dx.
__device__ inline void d_linear_backward(
    const float* w,
    float* dw,
    int out,
    int in,
    const float* x,
    const float* dy,
    float* dx) {
    d_outer_add(dw, dy, x, out, in);
    d_matvec_t(w, dy, dx, out, in);
}

// RMSNorm forward and backward, matching Python math.
__device__ inline void d_rmsnorm_forward(const float* x, float* y, float* inv_rms, int n) {
    float ms = 0.0f;
    for (int i = 0; i < n; ++i) {
        ms += x[i] * x[i];
    }
    ms /= static_cast<float>(n);
    *inv_rms = rsqrtf(ms + 1e-5f);
    for (int i = 0; i < n; ++i) {
        y[i] = x[i] * (*inv_rms);
    }
}

__device__ inline void d_rmsnorm_backward(
    const float* x,
    float inv_rms,
    const float* dy,
    float* dx,
    int n) {
    float dot = 0.0f;
    for (int i = 0; i < n; ++i) {
        dot += dy[i] * x[i];
    }
    float coeff = (inv_rms * inv_rms * inv_rms) / static_cast<float>(n);
    for (int i = 0; i < n; ++i) {
        dx[i] = dy[i] * inv_rms - x[i] * dot * coeff;
    }
}

// Stable softmax and fused CE used throughout train/eval.
__device__ inline void d_softmax(const float* logits, int n, float* probs) {
    float mx = logits[0];
    for (int i = 1; i < n; ++i) {
        if (logits[i] > mx) {
            mx = logits[i];
        }
    }
    float sum_exp = 0.0f;
    for (int i = 0; i < n; ++i) {
        probs[i] = expf(logits[i] - mx);
        sum_exp += probs[i];
    }
    float inv = 1.0f / sum_exp;
    for (int i = 0; i < n; ++i) {
        probs[i] *= inv;
    }
}

__device__ inline float d_cross_entropy_with_probs(const float* logits, int vocab, int target, float* probs) {
    float mx = logits[0];
    for (int i = 1; i < vocab; ++i) {
        if (logits[i] > mx) {
            mx = logits[i];
        }
    }

    float sum_exp = 0.0f;
    for (int i = 0; i < vocab; ++i) {
        probs[i] = expf(logits[i] - mx);
        sum_exp += probs[i];
    }
    float inv = 1.0f / sum_exp;
    for (int i = 0; i < vocab; ++i) {
        probs[i] *= inv;
    }

    return -(logits[target] - mx - logf(sum_exp));
}

__device__ inline void d_zero(float* x, int n) {
    for (int i = 0; i < n; ++i) {
        x[i] = 0.0f;
    }
}

// One-array AdamW update with bias correction and optional grad scaling.
__device__ inline void d_adamw_array(
    float* w,
    float* g,
    float* m,
    float* v,
    int n,
    float lr,
    float beta1,
    float beta2,
    float eps,
    float weight_decay,
    float one_minus_b1_prod,
    float one_minus_b2_prod,
    float grad_scale) {
    for (int i = 0; i < n; ++i) {
        float grad = g[i] * grad_scale;
        float mi = beta1 * m[i] + (1.0f - beta1) * grad;
        float vi = beta2 * v[i] + (1.0f - beta2) * grad * grad;
        m[i] = mi;
        v[i] = vi;
        float m_hat = mi / one_minus_b1_prod;
        float v_hat = vi / one_minus_b2_prod;
        w[i] -= lr * (m_hat / (sqrtf(v_hat) + eps) + weight_decay * w[i]);
        g[i] = 0.0f;
    }
}

__global__ void train_step_kernel(
    ModelPtrs model,
    const int* tokens,
    int n,
    float lr_t,
    float beta1,
    float beta2,
    float eps_adam,
    float weight_decay,
    float b1_prod,
    float b2_prod,
    float max_grad_norm,
    float* out_loss,
    float* out_grad_norm) {
    // Single-thread fused reference kernel: one launch = one full optimizer step.
    if (blockIdx.x != 0 || threadIdx.x != 0) {
        return;
    }

    if (n <= 0 || n > kBlockSize || model.vocab_size <= 0 || model.vocab_size > kMaxVocab) {
        out_loss[0] = nanf("");
        out_grad_norm[0] = nanf("");
        return;
    }

    const int vocab = model.vocab_size;
    const float inv_sqrt_head = 1.0f / sqrtf(static_cast<float>(kHeadDim));

    const int wte_size = vocab * kNEmbd;
    const int wpe_size = kBlockSize * kNEmbd;
    const int attn_size = kNEmbd * kNEmbd;
    const int fc1_size = kFcDim * kNEmbd;
    const int fc2_size = kNEmbd * kFcDim;

    d_zero(model.g_wte, wte_size);
    d_zero(model.g_wpe, wpe_size);
    d_zero(model.g_attn_wq, attn_size);
    d_zero(model.g_attn_wk, attn_size);
    d_zero(model.g_attn_wv, attn_size);
    d_zero(model.g_attn_wo, attn_size);
    d_zero(model.g_mlp_fc1, fc1_size);
    d_zero(model.g_mlp_fc2, fc2_size);

    float x_tokpos[kBlockSize][kNEmbd];
    float x0[kBlockSize][kNEmbd];
    float inv_rms0[kBlockSize];

    float x_resid_attn[kBlockSize][kNEmbd];
    float x_norm1[kBlockSize][kNEmbd];
    float inv_rms1[kBlockSize];

    float q[kBlockSize][kNEmbd];
    float k_cache[kBlockSize][kNEmbd];
    float v_cache[kBlockSize][kNEmbd];
    float attn_weights[kBlockSize][kNHead][kBlockSize];

    float x_attn[kBlockSize][kNEmbd];
    float x_after_attn[kBlockSize][kNEmbd];
    float x_norm2[kBlockSize][kNEmbd];
    float inv_rms2[kBlockSize];

    float fc1_pre[kBlockSize][kFcDim];
    float fc1_act[kBlockSize][kFcDim];
    float x_out[kBlockSize][kNEmbd];
    float x_final[kBlockSize][kNEmbd];

    float logits[kBlockSize][kMaxVocab];
    float probs[kBlockSize][kMaxVocab];

    float total_loss = 0.0f;

    // Forward pass over sequence positions: build activations and per-token CE.
    for (int pos = 0; pos < n; ++pos) {
        int token_id = tokens[pos];
        int target_id = tokens[pos + 1];

        for (int i = 0; i < kNEmbd; ++i) {
            x_tokpos[pos][i] =
                model.wte[token_id * kNEmbd + i] + model.wpe[pos * kNEmbd + i];
        }
        d_rmsnorm_forward(x_tokpos[pos], x0[pos], &inv_rms0[pos], kNEmbd);

        // 1) Attention block.
        d_vec_copy(x0[pos], x_resid_attn[pos], kNEmbd);
        d_rmsnorm_forward(x_resid_attn[pos], x_norm1[pos], &inv_rms1[pos], kNEmbd);

        d_matvec(model.attn_wq, x_norm1[pos], q[pos], kNEmbd, kNEmbd);
        d_matvec(model.attn_wk, x_norm1[pos], k_cache[pos], kNEmbd, kNEmbd);
        d_matvec(model.attn_wv, x_norm1[pos], v_cache[pos], kNEmbd, kNEmbd);

        d_zero(x_attn[pos], kNEmbd);
        for (int h = 0; h < kNHead; ++h) {
            int hs = h * kHeadDim;
            float attn_logits[kBlockSize];
            for (int t = 0; t <= pos; ++t) {
                float dot = 0.0f;
                for (int j = 0; j < kHeadDim; ++j) {
                    dot += q[pos][hs + j] * k_cache[t][hs + j];
                }
                attn_logits[t] = dot * inv_sqrt_head;
            }
            d_softmax(attn_logits, pos + 1, &attn_weights[pos][h][0]);

            for (int j = 0; j < kHeadDim; ++j) {
                float sum_v = 0.0f;
                for (int t = 0; t <= pos; ++t) {
                    sum_v += attn_weights[pos][h][t] * v_cache[t][hs + j];
                }
                x_attn[pos][hs + j] = sum_v;
            }
        }

        float wo_out[kNEmbd];
        d_matvec(model.attn_wo, x_attn[pos], wo_out, kNEmbd, kNEmbd);
        for (int i = 0; i < kNEmbd; ++i) {
            x_after_attn[pos][i] = wo_out[i] + x_resid_attn[pos][i];
        }

        // 2) MLP block.
        d_rmsnorm_forward(x_after_attn[pos], x_norm2[pos], &inv_rms2[pos], kNEmbd);
        d_matvec(model.mlp_fc1, x_norm2[pos], fc1_pre[pos], kFcDim, kNEmbd);
        for (int i = 0; i < kFcDim; ++i) {
            float z = fc1_pre[pos][i];
            fc1_act[pos][i] = z > 0.0f ? z * z : 0.0f;
        }
        float fc2_out[kNEmbd];
        d_matvec(model.mlp_fc2, fc1_act[pos], fc2_out, kNEmbd, kFcDim);
        for (int i = 0; i < kNEmbd; ++i) {
            x_out[pos][i] = fc2_out[i] + x_after_attn[pos][i];
            x_final[pos][i] = x_out[pos][i];
        }

        d_matvec(model.wte, x_final[pos], logits[pos], vocab, kNEmbd);
        total_loss += d_cross_entropy_with_probs(logits[pos], vocab, target_id, probs[pos]);
    }

    const float loss = total_loss / static_cast<float>(n);

    float dK[kBlockSize][kNEmbd];
    float dV[kBlockSize][kNEmbd];
    for (int t = 0; t < n; ++t) {
        d_zero(dK[t], kNEmbd);
        d_zero(dV[t], kNEmbd);
    }

    const float inv_n = 1.0f / static_cast<float>(n);

    // Backward pass through time: reverse sequence order.
    for (int pos = n - 1; pos >= 0; --pos) {
        int token_id = tokens[pos];
        int target_id = tokens[pos + 1];

        float dlogits[kMaxVocab];
        for (int i = 0; i < vocab; ++i) {
            dlogits[i] = probs[pos][i] * inv_n;
        }
        dlogits[target_id] -= inv_n;

        float d_x[kNEmbd];
        d_linear_backward(model.wte, model.g_wte, vocab, kNEmbd, x_final[pos], dlogits, d_x);

        // 2) MLP backward.
        float d_x_after_attn[kNEmbd];
        d_vec_copy(d_x, d_x_after_attn, kNEmbd);

        float d_fc1_act[kFcDim];
        d_linear_backward(model.mlp_fc2, model.g_mlp_fc2, kNEmbd, kFcDim, fc1_act[pos], d_x, d_fc1_act);

        float d_fc1_pre[kFcDim];
        for (int i = 0; i < kFcDim; ++i) {
            float z = fc1_pre[pos][i];
            d_fc1_pre[i] = z > 0.0f ? 2.0f * z * d_fc1_act[i] : 0.0f;
        }

        float d_x_norm2[kNEmbd];
        d_linear_backward(model.mlp_fc1, model.g_mlp_fc1, kFcDim, kNEmbd, x_norm2[pos], d_fc1_pre, d_x_norm2);

        float d_norm2_in[kNEmbd];
        d_rmsnorm_backward(x_after_attn[pos], inv_rms2[pos], d_x_norm2, d_norm2_in, kNEmbd);
        d_vec_add_inplace(d_x_after_attn, d_norm2_in, kNEmbd);

        // 1) Attention backward.
        float d_x_resid_attn[kNEmbd];
        d_vec_copy(d_x_after_attn, d_x_resid_attn, kNEmbd);

        float d_x_attn[kNEmbd];
        d_linear_backward(model.attn_wo, model.g_attn_wo, kNEmbd, kNEmbd, x_attn[pos], d_x_after_attn, d_x_attn);

        float dq[kNEmbd];
        d_zero(dq, kNEmbd);

        for (int h = 0; h < kNHead; ++h) {
            int hs = h * kHeadDim;
            float dweights[kBlockSize];
            for (int t = 0; t <= pos; ++t) {
                float dot = 0.0f;
                for (int j = 0; j < kHeadDim; ++j) {
                    dot += d_x_attn[hs + j] * v_cache[t][hs + j];
                }
                dweights[t] = dot;

                float wt = attn_weights[pos][h][t];
                for (int j = 0; j < kHeadDim; ++j) {
                    dV[t][hs + j] += wt * d_x_attn[hs + j];
                }
            }

            float sum_dw_w = 0.0f;
            for (int t = 0; t <= pos; ++t) {
                sum_dw_w += dweights[t] * attn_weights[pos][h][t];
            }

            for (int t = 0; t <= pos; ++t) {
                float wt = attn_weights[pos][h][t];
                float dlogit = wt * (dweights[t] - sum_dw_w);
                for (int j = 0; j < kHeadDim; ++j) {
                    dq[hs + j] += dlogit * k_cache[t][hs + j] * inv_sqrt_head;
                    dK[t][hs + j] += dlogit * q[pos][hs + j] * inv_sqrt_head;
                }
            }
        }

        float d_x_norm1[kNEmbd];
        d_zero(d_x_norm1, kNEmbd);
        float d_tmp[kNEmbd];

        d_linear_backward(model.attn_wq, model.g_attn_wq, kNEmbd, kNEmbd, x_norm1[pos], dq, d_tmp);
        d_vec_add_inplace(d_x_norm1, d_tmp, kNEmbd);

        d_linear_backward(model.attn_wk, model.g_attn_wk, kNEmbd, kNEmbd, x_norm1[pos], dK[pos], d_tmp);
        d_vec_add_inplace(d_x_norm1, d_tmp, kNEmbd);

        d_linear_backward(model.attn_wv, model.g_attn_wv, kNEmbd, kNEmbd, x_norm1[pos], dV[pos], d_tmp);
        d_vec_add_inplace(d_x_norm1, d_tmp, kNEmbd);

        float d_norm1_in[kNEmbd];
        d_rmsnorm_backward(x_resid_attn[pos], inv_rms1[pos], d_x_norm1, d_norm1_in, kNEmbd);

        for (int i = 0; i < kNEmbd; ++i) {
            d_x[i] = d_x_resid_attn[i] + d_norm1_in[i];
        }

        float d_tokpos[kNEmbd];
        d_rmsnorm_backward(x_tokpos[pos], inv_rms0[pos], d_x, d_tokpos, kNEmbd);
        for (int i = 0; i < kNEmbd; ++i) {
            model.g_wte[token_id * kNEmbd + i] += d_tokpos[i];
            model.g_wpe[pos * kNEmbd + i] += d_tokpos[i];
        }
    }

    // Gradient clipping by global norm.
    double sum_sq = 0.0;
    for (int i = 0; i < wte_size; ++i) {
        double g = model.g_wte[i];
        sum_sq += g * g;
    }
    for (int i = 0; i < wpe_size; ++i) {
        double g = model.g_wpe[i];
        sum_sq += g * g;
    }
    for (int i = 0; i < attn_size; ++i) {
        double gq = model.g_attn_wq[i];
        double gk = model.g_attn_wk[i];
        double gv = model.g_attn_wv[i];
        double go = model.g_attn_wo[i];
        sum_sq += gq * gq + gk * gk + gv * gv + go * go;
    }
    for (int i = 0; i < fc1_size; ++i) {
        double g = model.g_mlp_fc1[i];
        sum_sq += g * g;
    }
    for (int i = 0; i < fc2_size; ++i) {
        double g = model.g_mlp_fc2[i];
        sum_sq += g * g;
    }

    float gnorm = sqrtf(static_cast<float>(sum_sq));
    float grad_scale = 1.0f;
    if (gnorm > max_grad_norm) {
        grad_scale = max_grad_norm / gnorm;
    }

    float one_minus_b1_prod = 1.0f - b1_prod;
    float one_minus_b2_prod = 1.0f - b2_prod;

    // AdamW update for every parameter tensor.
    d_adamw_array(
        model.wte,
        model.g_wte,
        model.m_wte,
        model.v_wte,
        wte_size,
        lr_t,
        beta1,
        beta2,
        eps_adam,
        weight_decay,
        one_minus_b1_prod,
        one_minus_b2_prod,
        grad_scale);
    d_adamw_array(
        model.wpe,
        model.g_wpe,
        model.m_wpe,
        model.v_wpe,
        wpe_size,
        lr_t,
        beta1,
        beta2,
        eps_adam,
        weight_decay,
        one_minus_b1_prod,
        one_minus_b2_prod,
        grad_scale);
    d_adamw_array(
        model.attn_wq,
        model.g_attn_wq,
        model.m_attn_wq,
        model.v_attn_wq,
        attn_size,
        lr_t,
        beta1,
        beta2,
        eps_adam,
        weight_decay,
        one_minus_b1_prod,
        one_minus_b2_prod,
        grad_scale);
    d_adamw_array(
        model.attn_wk,
        model.g_attn_wk,
        model.m_attn_wk,
        model.v_attn_wk,
        attn_size,
        lr_t,
        beta1,
        beta2,
        eps_adam,
        weight_decay,
        one_minus_b1_prod,
        one_minus_b2_prod,
        grad_scale);
    d_adamw_array(
        model.attn_wv,
        model.g_attn_wv,
        model.m_attn_wv,
        model.v_attn_wv,
        attn_size,
        lr_t,
        beta1,
        beta2,
        eps_adam,
        weight_decay,
        one_minus_b1_prod,
        one_minus_b2_prod,
        grad_scale);
    d_adamw_array(
        model.attn_wo,
        model.g_attn_wo,
        model.m_attn_wo,
        model.v_attn_wo,
        attn_size,
        lr_t,
        beta1,
        beta2,
        eps_adam,
        weight_decay,
        one_minus_b1_prod,
        one_minus_b2_prod,
        grad_scale);
    d_adamw_array(
        model.mlp_fc1,
        model.g_mlp_fc1,
        model.m_mlp_fc1,
        model.v_mlp_fc1,
        fc1_size,
        lr_t,
        beta1,
        beta2,
        eps_adam,
        weight_decay,
        one_minus_b1_prod,
        one_minus_b2_prod,
        grad_scale);
    d_adamw_array(
        model.mlp_fc2,
        model.g_mlp_fc2,
        model.m_mlp_fc2,
        model.v_mlp_fc2,
        fc2_size,
        lr_t,
        beta1,
        beta2,
        eps_adam,
        weight_decay,
        one_minus_b1_prod,
        one_minus_b2_prod,
        grad_scale);

    out_loss[0] = loss;
    out_grad_norm[0] = gnorm;
}

__device__ inline void d_forward_token_logits(
    ModelPtrs model,
    const int* tokens,
    int pos,
    float k_cache[kBlockSize][kNEmbd],
    float v_cache[kBlockSize][kNEmbd],
    float* logits_out) {
    // Stateless per-token forward, reusing externally provided KV caches.
    const float inv_sqrt_head = 1.0f / sqrtf(static_cast<float>(kHeadDim));

    int token_id = tokens[pos];
    float x[kNEmbd];
    float x_norm[kNEmbd];
    float inv_rms = 0.0f;

    for (int i = 0; i < kNEmbd; ++i) {
        x[i] = model.wte[token_id * kNEmbd + i] + model.wpe[pos * kNEmbd + i];
    }
    d_rmsnorm_forward(x, x_norm, &inv_rms, kNEmbd);

    // 1) Attention block.
    float x_resid_attn[kNEmbd];
    d_vec_copy(x_norm, x_resid_attn, kNEmbd);

    float x_norm1[kNEmbd];
    d_rmsnorm_forward(x_resid_attn, x_norm1, &inv_rms, kNEmbd);

    float q[kNEmbd];
    d_matvec(model.attn_wq, x_norm1, q, kNEmbd, kNEmbd);
    d_matvec(model.attn_wk, x_norm1, k_cache[pos], kNEmbd, kNEmbd);
    d_matvec(model.attn_wv, x_norm1, v_cache[pos], kNEmbd, kNEmbd);

    float x_attn[kNEmbd];
    d_zero(x_attn, kNEmbd);
    for (int h = 0; h < kNHead; ++h) {
        int hs = h * kHeadDim;
        float attn_logits[kBlockSize];
        for (int t = 0; t <= pos; ++t) {
            float dot = 0.0f;
            for (int j = 0; j < kHeadDim; ++j) {
                dot += q[hs + j] * k_cache[t][hs + j];
            }
            attn_logits[t] = dot * inv_sqrt_head;
        }
        float attn_probs[kBlockSize];
        d_softmax(attn_logits, pos + 1, attn_probs);

        for (int j = 0; j < kHeadDim; ++j) {
            float sum_v = 0.0f;
            for (int t = 0; t <= pos; ++t) {
                sum_v += attn_probs[t] * v_cache[t][hs + j];
            }
            x_attn[hs + j] = sum_v;
        }
    }

    float wo_out[kNEmbd];
    d_matvec(model.attn_wo, x_attn, wo_out, kNEmbd, kNEmbd);
    float x_after_attn[kNEmbd];
    for (int i = 0; i < kNEmbd; ++i) {
        x_after_attn[i] = wo_out[i] + x_resid_attn[i];
    }

    // 2) MLP block.
    float x_norm2[kNEmbd];
    d_rmsnorm_forward(x_after_attn, x_norm2, &inv_rms, kNEmbd);
    float fc1_pre[kFcDim];
    d_matvec(model.mlp_fc1, x_norm2, fc1_pre, kFcDim, kNEmbd);
    float fc1_act[kFcDim];
    for (int i = 0; i < kFcDim; ++i) {
        float z = fc1_pre[i];
        fc1_act[i] = z > 0.0f ? z * z : 0.0f;
    }
    float fc2_out[kNEmbd];
    d_matvec(model.mlp_fc2, fc1_act, fc2_out, kNEmbd, kFcDim);
    float x_out[kNEmbd];
    for (int i = 0; i < kNEmbd; ++i) {
        x_out[i] = fc2_out[i] + x_after_attn[i];
    }

    // Weight tying: output projection reuses token embedding matrix.
    d_matvec(model.wte, x_out, logits_out, model.vocab_size, kNEmbd);
}

__global__ void eval_sequence_nll_kernel(ModelPtrs model, const int* tokens, int n, float* out_nll) {
    // Validation kernel: forward-only NLL accumulation, no gradient work.
    if (blockIdx.x != 0 || threadIdx.x != 0) {
        return;
    }

    if (n <= 0 || n > kBlockSize || model.vocab_size <= 0 || model.vocab_size > kMaxVocab) {
        out_nll[0] = nanf("");
        return;
    }

    float k_cache[kBlockSize][kNEmbd];
    float v_cache[kBlockSize][kNEmbd];
    float logits[kMaxVocab];
    float probs[kMaxVocab];

    float total_nll = 0.0f;
    for (int pos = 0; pos < n; ++pos) {
        d_forward_token_logits(model, tokens, pos, k_cache, v_cache, logits);
        int target = tokens[pos + 1];
        total_nll += d_cross_entropy_with_probs(logits, model.vocab_size, target, probs);
    }
    out_nll[0] = total_nll;
}

__global__ void forward_last_logits_kernel(ModelPtrs model, const int* tokens, int seq_len, float* out_logits) {
    // Inference kernel: run causal forward, return logits at final position.
    if (blockIdx.x != 0 || threadIdx.x != 0) {
        return;
    }

    if (seq_len <= 0 || seq_len > kBlockSize || model.vocab_size <= 0 || model.vocab_size > kMaxVocab) {
        for (int i = 0; i < model.vocab_size; ++i) {
            out_logits[i] = nanf("");
        }
        return;
    }

    float k_cache[kBlockSize][kNEmbd];
    float v_cache[kBlockSize][kNEmbd];
    float logits[kMaxVocab];

    for (int pos = 0; pos < seq_len; ++pos) {
        d_forward_token_logits(model, tokens, pos, k_cache, v_cache, logits);
    }

    for (int i = 0; i < model.vocab_size; ++i) {
        out_logits[i] = logits[i];
    }
}
int main(int argc, char** argv) {
    try {
        // This fused path is intentionally minimal and currently specialized to one layer.
        static_assert(kNLayer == 1, "current fused kernels are implemented for n_layer=1");

        // Let there be deterministic setup and data loading.
        TrainConfig cfg = parse_args(argc, argv);
        std::mt19937 rng(cfg.seed);
        DataBundle data = load_data(rng);

        if (data.train_docs.empty()) {
            throw std::runtime_error("no training documents after split");
        }
        if (static_cast<int>(data.itos.size()) > kMaxVocab) {
            throw std::runtime_error(
                "vocab size exceeds kMaxVocab. Increase kMaxVocab in microgpt_cuda.cu.");
        }

        DeviceModel model(static_cast<int>(data.itos.size()), rng);
        ModelPtrs model_ptrs = model.ptrs();

        DeviceBuffer<int> d_tokens(kMaxTokens);
        DeviceBuffer<float> d_loss(1);
        DeviceBuffer<float> d_grad_norm(1);
        DeviceBuffer<float> d_nll(1);
        DeviceBuffer<float> d_logits(static_cast<size_t>(model.vocab_size));

        std::cout << "num docs: " << data.docs.size()
                  << " (train: " << data.train_docs.size()
                  << ", val: " << data.val_docs.size() << ")\n";
        std::cout << "vocab size: " << data.itos.size() << "\n";
        std::cout << "num params: " << model.num_params() << "\n";

        // Adam running products for bias correction.
        float b1_prod = 1.0f;
        float b2_prod = 1.0f;
        constexpr float kPi = 3.14159265358979323846f;

        // Repeat in sequence: one document per step.
        for (int step = 0; step < cfg.num_steps; ++step) {
            auto t0 = std::chrono::high_resolution_clock::now();

            const std::string& doc =
                data.train_docs[static_cast<size_t>(step % static_cast<int>(data.train_docs.size()))];
            std::vector<int> tokens = encode_doc(doc, data.stoi, data.bos);
            int n = std::min(kBlockSize, static_cast<int>(tokens.size()) - 1);
            if (n <= 0) {
                continue;
            }

            d_tokens.upload_raw(tokens.data(), static_cast<size_t>(n + 1));

            // Cosine LR schedule, same functional form as microgpt.py.
            float lr_t = cfg.learning_rate * 0.5f * (1.0f + std::cos(kPi * step / cfg.num_steps));
            b1_prod *= cfg.beta1;
            b2_prod *= cfg.beta2;

            // One fused launch performs forward, backward, clipping, and AdamW.
            train_step_kernel<<<1, 1>>>(
                model_ptrs,
                d_tokens.data(),
                n,
                lr_t,
                cfg.beta1,
                cfg.beta2,
                cfg.eps_adam,
                cfg.weight_decay,
                b1_prod,
                b2_prod,
                cfg.max_grad_norm,
                d_loss.data(),
                d_grad_norm.data());
            CUDA_CHECK(cudaGetLastError());
            CUDA_CHECK(cudaDeviceSynchronize());

            float loss = 0.0f;
            d_loss.download_raw(&loss, 1);

            auto t1 = std::chrono::high_resolution_clock::now();
            auto ms = std::chrono::duration_cast<std::chrono::milliseconds>(t1 - t0).count();
            std::cout << "step " << std::setw(4) << (step + 1)
                      << "/" << std::setw(4) << cfg.num_steps
                      << " | loss " << std::fixed << std::setprecision(4) << loss
                      << " | " << ms << "ms\n";

            // Periodic validation on held-out docs.
            if ((step + 1) % cfg.val_every == 0) {
                float val_loss = 0.0f;
                int val_n = 0;
                int eval_docs = std::min(cfg.val_docs, static_cast<int>(data.val_docs.size()));
                for (int i = 0; i < eval_docs; ++i) {
                    std::vector<int> vt = encode_doc(data.val_docs[static_cast<size_t>(i)], data.stoi, data.bos);
                    int vn = std::min(kBlockSize, static_cast<int>(vt.size()) - 1);
                    if (vn <= 0) {
                        continue;
                    }
                    d_tokens.upload_raw(vt.data(), static_cast<size_t>(vn + 1));
                    eval_sequence_nll_kernel<<<1, 1>>>(model_ptrs, d_tokens.data(), vn, d_nll.data());
                    CUDA_CHECK(cudaGetLastError());
                    CUDA_CHECK(cudaDeviceSynchronize());

                    float nll = 0.0f;
                    d_nll.download_raw(&nll, 1);
                    val_loss += nll;
                    val_n += vn;
                }
                if (val_n > 0) {
                    std::cout << "  val loss: " << std::fixed << std::setprecision(4)
                              << (val_loss / static_cast<float>(val_n)) << "\n";
                } else {
                    std::cout << "  val loss: n/a\n";
                }
            }
        }

        // Inference: top-k sampled autoregressive decoding.
        std::cout << "\n--- inference ---\n";
        std::vector<float> host_logits(static_cast<size_t>(model.vocab_size), 0.0f);
        for (int sample_idx = 0; sample_idx < cfg.num_samples; ++sample_idx) {
            std::vector<int> seq;
            seq.push_back(data.bos);
            std::cout << "sample " << std::setw(2) << (sample_idx + 1) << ": ";

            for (int pos = 0; pos < kBlockSize; ++pos) {
                d_tokens.upload_raw(seq.data(), seq.size());
                forward_last_logits_kernel<<<1, 1>>>(
                    model_ptrs,
                    d_tokens.data(),
                    static_cast<int>(seq.size()),
                    d_logits.data());
                CUDA_CHECK(cudaGetLastError());
                CUDA_CHECK(cudaDeviceSynchronize());

                d_logits.download_raw(host_logits.data(), host_logits.size());
                int token_id = sample_top_k(host_logits, cfg.top_k, cfg.temperature, rng);
                if (token_id == data.bos) {
                    break;
                }

                std::cout << data.itos[static_cast<size_t>(token_id)];
                seq.push_back(token_id);
            }
            std::cout << "\n";
        }

        return 0;
    } catch (const std::exception& e) {
        std::cerr << "error: " << e.what() << "\n";
        return 1;
    }
}
